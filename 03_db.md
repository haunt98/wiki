# 03_db

## Redis

[How to do distributed locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html) vs
[Is Redlock safe?](http://antirez.com/news/101)

> Note that Redis uses gettimeofday, not a monotonic clock, to determine the expiry of keys. The man page for
> gettimeofday explicitly says that the time it returns is subject to discontinuous jumps in system time – that is, it
> might suddenly jump forwards by a few minutes, or even jump back in time
>
> If you need locks only on a best-effort basis (as an efficiency optimization, not for correctness), I would recommend
> sticking with the straightforward single-node locking algorithm for Redis (conditional set-if-not-exists to obtain a
> lock, atomic delete-if-value-matches to release a lock), and documenting very clearly in your code that the locks are
> only approximate and may occasionally fail. Don’t bother with setting up a cluster of five Redis nodes.

### References

- [Cache Consistency with Database](https://danielw.cn/cache-consistency-with-database#cache-patterns)
- [Are Redis ACL password protections weak?](https://blog.ovalerio.net/archives/2877)
- [I Made The World’s Best In-Memory Database But Forgot to Tell Anybody](https://matt.sh/best-database-ever)
- [Pagination With Redis](https://christophermcdowell.dev/post/pagination-with-redis/)
- [The Good, the Bad, and the Ugly Among Redis Pagination Strategies](https://dzone.com/articles/the-good-the-bad-and-the-ugly-among-redis-paginati)
- [Jepsen: Redis](https://aphyr.com/posts/283-call-me-maybe-redis)

## MySQL

[MySQL 5.7 Transaction Isolation Levels](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html)

> The isolation levels mentioned in the SQL standard are:
>
> - Serializable (most strict, expensive): A serializable execution produces the same effect as some serial execution of
>   those transactions. A serial execution is one in which each transaction executes to completion before the next
>   transaction begins. One note about Serializable level is that it is often implemented as “snapshot isolation” (e.g.
>   Oracle) due to differences in interpretation and “snapshot isolation” is not represented in the SQL standard.
> - Repeatable reads: Uncommitted reads in the current transaction are visible to the current transaction but changes
>   made by other transactions (such as newly inserted rows) won’t be visible.
> - Read committed: Uncommitted reads are not visible to the transactions. Only committed writes are visible but the
>   phantom reads may happen. If another transaction inserts and commits new rows, the current transaction can see them
>   when querying.
> - Read uncommitted (least strict, cheap): Dirty reads are allowed, transactions can see not-yet-committed changes made
>   by other transactions. In practice, this level could be useful to return approximate aggregates, such as COUNT(\*)
>   queries on a table.

- [InnoDB B-tree Latch Optimization History](http://baotiao.github.io/2024/06/09/english-btree.html)
- [Upgrading Uber’s MySQL Fleet to version 8.0](https://www.uber.com/en-JO/blog/upgrading-ubers-mysql-fleet/)

> In 5.6, during a write operation, if an SMO (structure modification operation) is in progress, the entire index->lock
> would be locked with an X lock. During this time, all read operations would be blocked.
>
> In 8.0, read operations and optimistic write operations are allowed to proceed during an SMO.
>
> However, in 8.0 there is still a limitation: only one SMO can occur at a time because the SX lock must be acquired
> during an SMO. Since SX locks conflict with other SX locks, this remains one of the main issues in 8.0.

### References

- [How to read MySQL EXPLAINs](https://planetscale.com/blog/how-read-mysql-explains)
- [MySQL B-tree Height Issues in Large Single Tables](http://baotiao.github.io/2024/09/04/btree-height-en.html)
- [MySQL transactions per second vs fsyncs per second](https://sirupsen.com/napkin/problem-10-mysql-transactions-per-second)
- [MySQL 8.0.34](https://jepsen.io/analyses/mysql-8.0.34)

## PostgreSQL

[My Notes on GitLab Postgres Schema Design](https://shekhargulati.com/2022/07/08/my-notes-on-gitlabs-postgres-schema-design/)

> Use of internal and external ids
>
> It is generally a good practice to not expose your primary keys to the external world. This is especially important
> when you use sequential auto-incrementing identifiers with type integer or bigint since they are guessable.

[The part of PostgreSQL we hate the most](https://ottertune.com/blog/the-part-of-postgresql-we-hate-the-most)

> The core idea of PostgreSQL’s MVCC scheme is seemingly straightforward: when a query updates an existing row in a
> table, the DBMS makes a copy of that row and applies the changes to this new version instead of overwriting the
> original row.

[Why Uber Engineering Switched from Postgres to MySQL](https://www.uber.com/en-VN/blog/postgres-to-mysql-migration/)

> - Inefficient architecture for writes
> - Inefficient data replication
> - Issues with table corruption
> - Poor replica MVCC support
> - Difficulty upgrading to newer releases
>
> With Postgres, the primary index and secondary indexes all point directly to the on-disk tuple offsets. When a tuple
> location changes, all indexes must be updated.

### References

- [How Postgres stores data on disk – this one's a page turner](https://drew.silcock.dev/blog/how-postgres-stores-data-on-disk/)
- [How Postgres stores oversized values – let's raise a TOAST](https://drew.silcock.dev/blog/how-postgres-stores-oversized-values/)
- [How Postgres Makes Transactions Atomic](https://brandur.org/postgres-atomicity)
- [Nine ways to shoot yourself in the foot with PostgreSQL](https://philbooth.me/blog/nine-ways-to-shoot-yourself-in-the-foot-with-postgresql)
- [Query best practices: When should you use the IN instead of the OR operator?](https://ottertune.com/blog/query-best-practices-when-should-you-use-the-in-instead-of-the-or-operator)
- [Explaining The Postgres Meme](https://www.avestura.dev/blog/explaining-the-postgres-meme)
- [Understanding the Postgres Hackers Mailing List Language](https://www.crunchydata.com/blog/understanding-the-postgres-hackers-mailing-list)
- [As Rails developers, why we are excited about PostgreSQL 17](https://dev.to/lifen/as-rails-developers-why-we-are-excited-about-postgresql-17-27nj)
- [The Part of PostgreSQL We Hate the Most](https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html)
- [Schema changes and the Postgres lock queue](https://xata.io/blog/migrations-and-exclusive-locks)
- [Transaction Isolation in Postgres, explained](https://www.thenile.dev/blog/transaction-isolation-postgres)
- [Postgres: Boundless `text` and Back Again](https://brandur.org/text)
- [Life Altering Postgresql Patterns](https://mccue.dev/pages/3-11-25-life-altering-postgresql-patterns)
- [VectorChord-BM25: Revolutionize PostgreSQL Search with BM25 Ranking — 3x Faster Than ElasticSearch](https://blog.vectorchord.ai/vectorchord-bm25-revolutionize-postgresql-search-with-bm25-ranking-3x-faster-than-elasticsearch)
- [Waiting for Postgres 18: Accelerating Disk Reads with Asynchronous I/O](https://pganalyze.com/blog/postgres-18-async-io)
- [PostgreSQL 12.3](https://jepsen.io/analyses/postgresql-12.3)

- [How Notion Sharded Their Postgres Database](https://blog.quastor.org/p/notion-sharded-postgres-database-8af4)
- [How Figma’s databases team lived to tell the scale](https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/)
    - [Why Has Figma Reinvented the Wheel With PostgreSQL?](https://medium.com/@magda7817/why-has-figma-reinveted-the-wheel-with-postgresql-3a1cb2e9297c)
- [Migrating billions of records: moving our active DNS database while it’s in use](https://blog.cloudflare.com/migrating-billions-of-records-moving-our-active-dns-database-while-in-use/)

## Kafka

[Kafka: Design](https://kafka.apache.org/documentation.html#design)

[The Log: What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)

> A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records
> ordered by time. It looks like this:
>
> Records are appended to the end of the log, and reads proceed left-to-right. Each entry is assigned a unique
> sequential log entry number.
>
> Over-time the usage of the log grew from an implementation detail of ACID to a method for replicating data between
> databases. It turns out that the sequence of changes that happened on the database is exactly what is needed to keep a
> remote replica database in sync. Oracle, MySQL, and PostgreSQL include log shipping protocols to transmit portions of
> log to replica databases which act as slaves.
>
> The two problems a log solves—ordering changes and distributing data—are even more important in distributed data
> systems.
>
> We used a few tricks in Kafka to support this kind of scale:
>
> - Partitioning the log
> - Optimizing throughput by batching reads and writes
> - Avoiding needless data copies
>
> Each partition is a totally ordered log, but there is no global ordering between partitions (other than perhaps some
> wall-clock time you might include in your messages). The assignment of the messages to a particular partition is
> controllable by the writer, with most users choosing to partition by some kind of key (e.g. user id). Partitioning
> allows log appends to occur without co-ordination between shards and allows the throughput of the system to scale
> linearly with the Kafka cluster size.
>
> Each partition is replicated across a configurable number of replicas, each of which has an identical copy of the
> partition's log. At any time, a single one of them will act as the leader; if the leader fails, one of the replicas
> will take over as leader.
>
> Lack of a global order across partitions is a limitation, but we have not found it to be a major one. Indeed,
> interaction with the log typically comes from hundreds or thousands of distinct processes so it is not meaningful to
> talk about a total order over their behavior. Instead, the guarantees that we provide are that each partition is order
> preserving, and Kafka guarantees that appends to a particular partition from a single sender will be delivered in the
> order they are sent.
>
> A log, like a filesystem, is easy to optimize for linear read and write patterns. The log can group small reads and
> writes together into larger, high-throughput operations. Kafka pursues this optimization aggressively. Batching occurs
> from client to server when sending data, in writes to disk, in replication between servers, in data transfer to
> consumers, and in acknowledging committed data.
>
> Finally, Kafka uses a simple binary format that is maintained between in-memory log, on-disk log, and in network data
> transfers. This allows us to make use of numerous optimizations including zero-copy data transfer.
>
> Second, the log provides buffering to the processes. This is very fundamental. If processing proceeds in an
> unsynchronized fashion it is likely to happen that an upstream data producing job will produce data more quickly than
> another downstream job can consume it. When this occurs processing must block, buffer or drop data. Dropping data is
> likely not an option; blocking may cause the entire processing graph to grind to a halt. The log acts as a very, very
> large buffer that allows process to be restarted or fail without slowing down other parts of the processing graph.
> This isolation is particularly important when extending this data flow to a larger organization, where processing is
> happening by jobs made by many different teams. We cannot have one faulty job cause back-pressure that stops the
> entire processing flow.

- [How Kafka Is so Performant If It Writes to Disk?](https://andriymz.github.io/kafka/kafka-disk-write-performance/)

- [Queues in Apache Kafka®: Enhancing Message Processing and Scalability](https://www.confluent.io/blog/queues-on-kafka/)
- [KIP-932: Queues for Kafka](https://cwiki.apache.org/confluence/display/KAFKA/KIP-932%3A+Queues+for+Kafka)
- [Kafka at the low end: how bad can it get?](https://broot.ca/kafka-at-the-low-end.html)

> Key characteristics of a queue:
>
> Asynchronous communication: In our coffee shop example, you don’t need to stand there and watch your coffee being
> made. You can carry on with your day while waiting for your order to be ready. Similarly, message queues allow
> producers (like your coffee ordering app) to send messages via the queue without needing the consumers (baristas) to
> be ready at the same time.
>
> First In, First Out (FIFO): Just like the order in which you place your orders matters, message queues typically
> follow a FIFO pattern. The first message sent to the queue is the first one to be processed. This is crucial for
> applications that rely on the order of operations, like processing transactions in a banking system. There are shared
> queues where the ordering is not guaranteed. We will talk about that later in the blog.
>
> Durability: Most message queues ensure that messages are stored reliably. Even if the system crashes or a consumer
> goes offline, the messages will wait patiently in the queue until they can be processed.
>
> Message deletion: Messages are stored in the queue until a consumer processes them. Each message is typically consumed
> by one consumer instance, ensuring exclusive delivery. Messages are deleted after being consumed (acknowledged) by a
> consumer. This ensures that no duplicate processing occurs, and the queue is cleaned up over time.

> Key characteristics of streaming messages:
>
> Real-time processing: Unlike our coffee shop scenario, streaming messages are designed for immediate consumption.
> Think of it as listening to music on a streaming service. You get instant access to songs as they’re played without
> waiting for the entire album to download.
>
> Event-driven architecture: Streaming is all about reacting to events as they happen. If you’ve ever used a social
> media platform, you’ve experienced streaming in action. Your feed updates in real time, showing you the latest posts,
> likes, and comments. This is how streaming messages work—pushing data to consumers as soon as it’s available.
>
> Scalability: Streaming systems can handle a massive volume of data, processing thousands of messages per second.
> They’re perfect for applications like real-time analytics, monitoring, and machine learning.
>
> Message retention: Messages are stored and can be consumed by multiple consumers, enabling both real-time and batch
> processing. Consumers track their progress (via offsets), allowing flexible replay of messages. Messages can be
> deleted based on retention policies, such as time-based (e.g., retain for 7 days) or size-based limits (e.g., retain
> up to 1GB per partition). Consumers do not trigger message deletion; rather, data is retained for historical access or
> reprocessing.

### References

- [In-Stream Big Data Processing](https://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/)
- [Apache Kafka® Quick Start](https://developer.confluent.io/quickstart/kafka-local/)
- [Apache Kafka Needs No Keeper: Removing the Apache ZooKeeper Dependency](https://www.confluent.io/blog/removing-zookeeper-dependency-in-kafka/)
    - [KRaft: Apache Kafka Without ZooKeeper](https://developer.confluent.io/learn/kraft/)
- [KIP-848: The Next Generation of the Consumer Rebalance Protocol](https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol)
- [Jepsen: Kafka](https://aphyr.com/posts/293-call-me-maybe-kafka)

## SQLite

[Repairing database on the fly for millions of users](https://ashishb.net/programming/repair-database-on-mobile-device/)

> -- The .bail off command tells SQLite to continue executing statements even if errors occur .bail off

- [Collection of insane and fun facts about SQLite](https://avi.im/blag/2024/sqlite-facts/)

## Other

[B-trees and database indexes](https://planetscale.com/blog/btrees-and-database-indexes)

> Why are B+ trees better for databases? There are two primary reasons.
>
> - Since inner nodes do not have to store values, we can fit more keys per inner node! This can help keep the tree
>   shallower.
> - All of the values can be stored at the same level, and traversed in-order via the bottom-level linked list.
>
> Here, we mostly focused on comparing a sequential key to a random / UUID key. However, the principles shown here are
> useful to keep in mind no matter what kind of primary or secondary key you are considering.
>
> For example, you may also consider using a user.created_at timestamp as a key for an index. This will have similar
> properties to a sequential integer. Insertions will generally always go to the right-most path, unless legacy data is
> being inserted.
>
> Conversely, something like a user.email_address string will have more similar characteristics to a random key. Users
> won't be creating accounts in email-alphabetical order, so insertions will happen all over the place in the B+tree.

- [Why Databases Write Ahead](https://aneesh.mataroa.blog/blog/why-databases-write-ahead/)
- [A write-ahead log is not a universal part of durability](https://notes.eatonphil.com/2024-07-01-a-write-ahead-log-is-not-a-universal-part-of-durability.html)

Life without WAL.

The problem: Updating the on-disk data structures is costly, performance-wise. Firstly, finding the OS page where the
record/index entry is stored translates to random IO. Secondly, to ensure 100% durability, one would have to ensure that
the file changes are actually synced to disk, and not just living in the OS/disk cache, which is again, not free of
cost. See fsync!

Without WAL: CRUD database → wait to sync to disk → another CRUD

With WAL:

- CRUD database → another CRUD → … → write to log
- Another process get from log → sync to disk

WAL: enable replication

### References

- [Things I Wished More Developers Knew About Databases](https://rakyll.medium.com/things-i-wished-more-developers-knew-about-databases-2d0178464f78)
- [Things You Should Know About Databases](https://architecturenotes.co/things-you-should-know-about-databases/)
- [Optimistic and pessimistic locking with SQL](https://convincedcoder.com/2018/09/01/Optimistic-pessimistic-locking-sql/)
- [How does B-tree make your queries fast?](https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html)
- [Demystifying Database Transactions](https://dineshgowda.com/posts/demystifying-database-transcations/)
- [DELETEs are difficult](https://notso.boringsql.com/posts/deletes-are-difficult/)
- [B-Trees: More Than I Thought I'd Want to Know](https://benjamincongdon.me/blog/2021/08/17/B-Trees-More-Than-I-Thought-Id-Want-to-Know/)
- [Revisiting B+-tree vs. LSM-tree](https://www.usenix.org/publications/loginonline/revisit-b-tree-vs-lsm-tree-upon-arrival-modern-storage-hardware-built)
- [Online migrations at scale](https://stripe.com/blog/online-migrations)

- [A Closer Look to a Key-Value Storage Engine](https://silhding.github.io/2021/08/20/A-Closer-Look-to-a-Key-Value-Storage-Engine/)
- [Things that go wrong with disk IO](https://notes.eatonphil.com/2025-03-27-things-that-go-wrong-with-disk-io.html)
- [LSM in a Week](https://skyzh.github.io/mini-lsm/00-preface.html)
- [How RocksDB works](https://artem.krylysov.com/blog/2023/04/19/how-rocksdb-works/)
- [How CouchDB Prevents Data Corruption: fsync](https://neighbourhood.ie/blog/2025/02/26/how-couchdb-prevents-data-corruption-fsync)
- [TigerBeetle 0.16.11](https://jepsen.io/analyses/tigerbeetle-0.16.11)
