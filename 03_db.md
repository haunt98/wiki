# 03_db

## Redis

[How to do distributed locking](https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html)
vs [Is Redlock safe?](http://antirez.com/news/101)

> Note that Redis uses gettimeofday, not a monotonic clock, to determine the
> expiry of keys. The man page for gettimeofday explicitly says that the time it
> returns is subject to discontinuous jumps in system time – that is, it might
> suddenly jump forwards by a few minutes, or even jump back in time
>
> If you need locks only on a best-effort basis (as an efficiency optimization,
> not for correctness), I would recommend sticking with the straightforward
> single-node locking algorithm for Redis (conditional set-if-not-exists to
> obtain a lock, atomic delete-if-value-matches to release a lock), and
> documenting very clearly in your code that the locks are only approximate and
> may occasionally fail. Don’t bother with setting up a cluster of five Redis
> nodes.

### References

- [Cache Consistency with Database](https://danielw.cn/cache-consistency-with-database#cache-patterns)
- [Are Redis ACL password protections weak?](https://blog.ovalerio.net/archives/2877)
- [I Made The World’s Best In-Memory Database But Forgot to Tell Anybody](https://matt.sh/best-database-ever)

- [Pagination With Redis](https://christophermcdowell.dev/post/pagination-with-redis/)
- [The Good, the Bad, and the Ugly Among Redis Pagination Strategies](https://dzone.com/articles/the-good-the-bad-and-the-ugly-among-redis-paginati)

## MySQL

[MySQL 5.7 Transaction Isolation Levels](https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html)

> The isolation levels mentioned in the SQL standard are:
>
> - Serializable (most strict, expensive): A serializable execution produces the
>   same effect as some serial execution of those transactions. A serial
>   execution is one in which each transaction executes to completion before the
>   next transaction begins. One note about Serializable level is that it is
>   often implemented as “snapshot isolation” (e.g. Oracle) due to differences
>   in interpretation and “snapshot isolation” is not represented in the SQL
>   standard.
> - Repeatable reads: Uncommitted reads in the current transaction are visible
>   to the current transaction but changes made by other transactions (such as
>   newly inserted rows) won’t be visible.
> - Read committed: Uncommitted reads are not visible to the transactions. Only
>   committed writes are visible but the phantom reads may happen. If another
>   transaction inserts and commits new rows, the current transaction can see
>   them when querying.
> - Read uncommitted (least strict, cheap): Dirty reads are allowed,
>   transactions can see not-yet-committed changes made by other transactions.
>   In practice, this level could be useful to return approximate aggregates,
>   such as COUNT(*) queries on a table.

- [InnoDB B-tree Latch Optimization History](http://baotiao.github.io/2024/06/09/english-btree.html)
- [Upgrading Uber’s MySQL Fleet to version 8.0](https://www.uber.com/en-JO/blog/upgrading-ubers-mysql-fleet/)

> In 5.6, during a write operation, if an SMO (structure modification operation)
> is in progress, the entire index->lock would be locked with an X lock. During
> this time, all read operations would be blocked.
>
> In 8.0, read operations and optimistic write operations are allowed to proceed
> during an SMO.
>
> However, in 8.0 there is still a limitation: only one SMO can occur at a time
> because the SX lock must be acquired during an SMO. Since SX locks conflict
> with other SX locks, this remains one of the main issues in 8.0.

### References

- [How to read MySQL EXPLAINs](https://planetscale.com/blog/how-read-mysql-explains)
- [MySQL B-tree Height Issues in Large Single Tables](http://baotiao.github.io/2024/09/04/btree-height-en.html)

## PostgreSQL

[My Notes on GitLab Postgres Schema Design](https://shekhargulati.com/2022/07/08/my-notes-on-gitlabs-postgres-schema-design/)

> Use of internal and external ids
>
> It is generally a good practice to not expose your primary keys to the
> external world. This is especially important when you use sequential
> auto-incrementing identifiers with type integer or bigint since they are
> guessable.

[The part of PostgreSQL we hate the most](https://ottertune.com/blog/the-part-of-postgresql-we-hate-the-most)

> The core idea of PostgreSQL’s MVCC scheme is seemingly straightforward: when a
> query updates an existing row in a table, the DBMS makes a copy of that row
> and applies the changes to this new version instead of overwriting the
> original row.

[Why Uber Engineering Switched from Postgres to MySQL](https://www.uber.com/en-VN/blog/postgres-to-mysql-migration/)

> - Inefficient architecture for writes
> - Inefficient data replication
> - Issues with table corruption
> - Poor replica MVCC support
> - Difficulty upgrading to newer releases
>
> With Postgres, the primary index and secondary indexes all point directly to
> the on-disk tuple offsets. When a tuple location changes, all indexes must be
> updated.

### References

- [How Postgres stores data on disk – this one's a page turner](https://drew.silcock.dev/blog/how-postgres-stores-data-on-disk/)
- [How Postgres stores oversized values – let's raise a TOAST](https://drew.silcock.dev/blog/how-postgres-stores-oversized-values/)
- [How Postgres Makes Transactions Atomic](https://brandur.org/postgres-atomicity)
- [Nine ways to shoot yourself in the foot with PostgreSQL](https://philbooth.me/blog/nine-ways-to-shoot-yourself-in-the-foot-with-postgresql)
- [Query best practices: When should you use the IN instead of the OR operator?](https://ottertune.com/blog/query-best-practices-when-should-you-use-the-in-instead-of-the-or-operator)
- [Explaining The Postgres Meme](https://www.avestura.dev/blog/explaining-the-postgres-meme)
- [Understanding the Postgres Hackers Mailing List Language](https://www.crunchydata.com/blog/understanding-the-postgres-hackers-mailing-list)
- [As Rails developers, why we are excited about PostgreSQL 17](https://dev.to/lifen/as-rails-developers-why-we-are-excited-about-postgresql-17-27nj)
- [The Part of PostgreSQL We Hate the Most](https://www.cs.cmu.edu/~pavlo/blog/2023/04/the-part-of-postgresql-we-hate-the-most.html)
- [Schema changes and the Postgres lock queue](https://xata.io/blog/migrations-and-exclusive-locks)
- [Transaction Isolation in Postgres, explained](https://www.thenile.dev/blog/transaction-isolation-postgres)

- [How Notion Sharded Their Postgres Database](https://blog.quastor.org/p/notion-sharded-postgres-database-8af4)
- [How Figma’s databases team lived to tell the scale](https://www.figma.com/blog/how-figmas-databases-team-lived-to-tell-the-scale/)
  - [Why Has Figma Reinvented the Wheel With PostgreSQL?](https://medium.com/@magda7817/why-has-figma-reinveted-the-wheel-with-postgresql-3a1cb2e9297c)
- [Migrating billions of records: moving our active DNS database while it’s in use](https://blog.cloudflare.com/migrating-billions-of-records-moving-our-active-dns-database-while-in-use/)

## Kafka

[Kafka: Design](https://kafka.apache.org/documentation.html#design)

[The Log: What every software engineer should know about real-time data's unifying abstraction](https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying)

> A log is perhaps the simplest possible storage abstraction. It is an
> append-only, totally-ordered sequence of records ordered by time. It looks
> like this:
>
> Records are appended to the end of the log, and reads proceed left-to-right.
> Each entry is assigned a unique sequential log entry number.
>
> Over-time the usage of the log grew from an implementation detail of ACID to a
> method for replicating data between databases. It turns out that the sequence
> of changes that happened on the database is exactly what is needed to keep a
> remote replica database in sync. Oracle, MySQL, and PostgreSQL include log
> shipping protocols to transmit portions of log to replica databases which act
> as slaves.
>
> The two problems a log solves—ordering changes and distributing data—are even
> more important in distributed data systems.
>
> We used a few tricks in Kafka to support this kind of scale:
>
> - Partitioning the log
> - Optimizing throughput by batching reads and writes
> - Avoiding needless data copies
>
> Each partition is a totally ordered log, but there is no global ordering
> between partitions (other than perhaps some wall-clock time you might include
> in your messages). The assignment of the messages to a particular partition is
> controllable by the writer, with most users choosing to partition by some kind
> of key (e.g. user id). Partitioning allows log appends to occur without
> co-ordination between shards and allows the throughput of the system to scale
> linearly with the Kafka cluster size.
>
> Each partition is replicated across a configurable number of replicas, each of
> which has an identical copy of the partition's log. At any time, a single one
> of them will act as the leader; if the leader fails, one of the replicas will
> take over as leader.
>
> Lack of a global order across partitions is a limitation, but we have not
> found it to be a major one. Indeed, interaction with the log typically comes
> from hundreds or thousands of distinct processes so it is not meaningful to
> talk about a total order over their behavior. Instead, the guarantees that we
> provide are that each partition is order preserving, and Kafka guarantees that
> appends to a particular partition from a single sender will be delivered in
> the order they are sent.
>
> A log, like a filesystem, is easy to optimize for linear read and write
> patterns. The log can group small reads and writes together into larger,
> high-throughput operations. Kafka pursues this optimization aggressively.
> Batching occurs from client to server when sending data, in writes to disk, in
> replication between servers, in data transfer to consumers, and in
> acknowledging committed data.
>
> Finally, Kafka uses a simple binary format that is maintained between
> in-memory log, on-disk log, and in network data transfers. This allows us to
> make use of numerous optimizations including zero-copy data transfer.
>
> Second, the log provides buffering to the processes. This is very fundamental.
> If processing proceeds in an unsynchronized fashion it is likely to happen
> that an upstream data producing job will produce data more quickly than
> another downstream job can consume it. When this occurs processing must block,
> buffer or drop data. Dropping data is likely not an option; blocking may cause
> the entire processing graph to grind to a halt. The log acts as a very, very
> large buffer that allows process to be restarted or fail without slowing down
> other parts of the processing graph. This isolation is particularly important
> when extending this data flow to a larger organization, where processing is
> happening by jobs made by many different teams. We cannot have one faulty job
> cause back-pressure that stops the entire processing flow.

- [How Kafka Is so Performant If It Writes to Disk?](https://andriymz.github.io/kafka/kafka-disk-write-performance/)

### References

- [In-Stream Big Data Processing](https://highlyscalable.wordpress.com/2013/08/20/in-stream-big-data-processing/)

- [Apache Kafka® Quick Start](https://developer.confluent.io/quickstart/kafka-local/)
- [Apache Kafka Needs No Keeper: Removing the Apache ZooKeeper Dependency](https://www.confluent.io/blog/removing-zookeeper-dependency-in-kafka/)
  - [KRaft: Apache Kafka Without ZooKeeper](https://developer.confluent.io/learn/kraft/)

## SQLite

[Optimizing SQLite for servers](https://kerkour.com/sqlite-for-servers)

```sql
PRAGMA journal_mode = WAL;
PRAGMA busy_timeout = 5000;
PRAGMA synchronous = NORMAL;
PRAGMA cache_size = 1000000000;
PRAGMA foreign_keys = true;
PRAGMA temp_store = memory;
```

[Repairing database on the fly for millions of users](https://ashishb.net/programming/repair-database-on-mobile-device/)

> -- The .bail off command tells SQLite to continue executing statements even if
> errors occur .bail off

## Other

[B-trees and database indexes](https://planetscale.com/blog/btrees-and-database-indexes)

> Why are B+ trees better for databases? There are two primary reasons.
>
> - Since inner nodes do not have to store values, we can fit more keys per
>   inner node! This can help keep the tree shallower.
> - All of the values can be stored at the same level, and traversed in-order
>   via the bottom-level linked list.
>
> Here, we mostly focused on comparing a sequential key to a random / UUID key.
> However, the principles shown here are useful to keep in mind no matter what
> kind of primary or secondary key you are considering.
>
> For example, you may also consider using a user.created_at timestamp as a key
> for an index. This will have similar properties to a sequential integer.
> Insertions will generally always go to the right-most path, unless legacy data
> is being inserted.
>
> Conversely, something like a user.email_address string will have more similar
> characteristics to a random key. Users won't be creating accounts in
> email-alphabetical order, so insertions will happen all over the place in the
> B+tree.

- [Why Databases Write Ahead](https://aneesh.mataroa.blog/blog/why-databases-write-ahead/)
- [A write-ahead log is not a universal part of durability](https://notes.eatonphil.com/2024-07-01-a-write-ahead-log-is-not-a-universal-part-of-durability.html)

Life without WAL.

The problem: Updating the on-disk data structures is costly, performance-wise.
Firstly, finding the OS page where the record/index entry is stored translates
to random IO. Secondly, to ensure 100% durability, one would have to ensure that
the file changes are actually synced to disk, and not just living in the OS/disk
cache, which is again, not free of cost. See fsync!

Without WAL: CRUD database → wait to sync to disk → another CRUD

With WAL:

- CRUD database → another CRUD → … → write to log
- Another process get from log → sync to disk

WAL: enable replication

### References

- [Things I Wished More Developers Knew About Databases](https://rakyll.medium.com/things-i-wished-more-developers-knew-about-databases-2d0178464f78)
- [Things You Should Know About Databases](https://architecturenotes.co/things-you-should-know-about-databases/)
- [Optimistic and pessimistic locking with SQL](https://convincedcoder.com/2018/09/01/Optimistic-pessimistic-locking-sql/)
- [How does B-tree make your queries fast?](https://blog.allegro.tech/2023/11/how-does-btree-make-your-queries-fast.html)
- [Demystifying Database Transactions](https://dineshgowda.com/posts/demystifying-database-transcations/)
- [DELETEs are difficult](https://notso.boringsql.com/posts/deletes-are-difficult/)

- [How RocksDB works](https://artem.krylysov.com/blog/2023/04/19/how-rocksdb-works/)
- [A Closer Look to a Key-Value Storage Engine](https://silhding.github.io/2021/08/20/A-Closer-Look-to-a-Key-Value-Storage-Engine/)
- [LSM in a Week](https://skyzh.github.io/mini-lsm/00-preface.html)
- [Revisiting B+-tree vs. LSM-tree](https://www.usenix.org/publications/loginonline/revisit-b-tree-vs-lsm-tree-upon-arrival-modern-storage-hardware-built)

- [Online migrations at scale](https://stripe.com/blog/online-migrations)
