# 04_ai

## Founding papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Interesting

- [What’s Really Going On in Machine Learning? Some Minimal Models](https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/)
- [Generative AI Space and the Mental Imagery of Alien Minds](https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/)
- [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

- [Retrieval augmented generation: Keeping LLMs relevant and current](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/)
- [Introduction to Weight Quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c)
  - FP32: 1 sign bit, 8 exponent bits, 23 significand bits
  - FP16: 1 sign bit, 5 exponent bits, 10 significand bits
  - BF16: 1 sign bit, 8 exponent bits, 7 significand bits -> prevent underflow,
    overflow of FP16
- [4-bit Quantization with GPTQ](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34)
- [Quantize Llama models with GGUF and llama.cpp](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172)
- [Quantization of LLMs with llama.cpp](https://medium.com/@ingridwickstevens/quantization-of-llms-with-llama-cpp-9bbf59deda35)
  - Q4_K_M: 4-bit quantization with K-means clustering with M aka medium size
- [Transformers Laid Out](https://goyalpramod.github.io/blogs/Transformers_laid_out/)
- [Very simple to understand: RoPE, 2D-RoPE, M-RoPE](https://blog.ngxson.com/very-simple-to-understand-rope-2drope-mrope)

- [I accidentally built a meme search engine](https://harper.blog/2024/04/12/i-accidentally-built-a-meme-search-engine/)
- [Don't use cosine similarity carelessly](https://p.migdal.pl/blog/2025/01/dont-use-cosine-similarity)
- [Markov Chain Monte Carlo Without all the Bullshit](https://www.jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/)

- [Interpretable Machine Learning - A Guide for Making Black Box Models Explainable](https://christophm.github.io/interpretable-ml-book/)
- [What is the Shapley value ?](https://medium.com/the-modern-scientist/what-is-the-shapley-value-8ca624274d5a)
- [Explaining machine learning models with SHAP and SAGE](https://iancovert.com/blog/understanding-shap-sage/)
