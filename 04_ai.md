# 04_ai

## Founding papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Interesting

- [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
- [What’s Really Going On in Machine Learning? Some Minimal Models](https://confluence.zalopay.vn/pages/viewpage.action?spaceKey=PD&title=%5Bekyc+x+TrueID%5D+ekyc+decision)

- [Dive into Deep Learning](https://d2l.ai/index.html)
- [UvA Deep Learning Tutorials](https://uvadlc-notebooks.readthedocs.io/en/latest/)
- [Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/)
- [Alternatives to a Degree to Prove Yourself in Deep Learning](https://www.fast.ai/posts/2017-04-06-alternatives.html)

- [Introduction to Weight Quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c)
  - FP32: 1 sign bit, 8 exponent bits, 23 significand bits
  - FP16: 1 sign bit, 5 exponent bits, 10 significand bits
  - BF16: 1 sign bit, 8 exponent bits, 7 significand bits -> prevent underflow,
    overflow of FP16
- [4-bit Quantization with GPTQ](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34)
- [Quantize Llama models with GGUF and llama.cpp](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172)
- [Quantization of LLMs with llama.cpp](https://medium.com/@ingridwickstevens/quantization-of-llms-with-llama-cpp-9bbf59deda35)
  - Q4_K_M: 4-bit quantization with K-means clustering with M aka medium size

- [How I Use "AI"](https://nicholas.carlini.com/writing/2024/how-i-use-ai.html)
- https://www.reddit.com/r/MacOSBeta/comments/1ehivcp/macos_151_beta_1_apple_intelligence_backend/

- [How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/)
- https://github.com/arpita8/Awesome-Mixture-of-Experts-Papers
- https://github.com/unslothai/unsloth

## Code pool

- [Tensors from Scratch #1: Tensors and their Shapes](https://maharshi.bearblog.dev/tensors-from-scratch-part-1/)
- [Tensors from Scratch #2: Elementwise operations and Broadcasting](https://maharshi.bearblog.dev/tensors-from-scratch-part-2/)
- [Informal approach to attention mechanisms](https://maharshi.bearblog.dev/informal-approach-to-attention-in-transformers/)
- [Generative transformer from first principles in Julia](https://liorsinai.github.io/machine-learning/2024/03/23/transformers-gpt.html)
- [The Path to StyleGan2 - Implementing the Progressive Growing GAN](https://ym2132.github.io/Progressive_GAN)

- https://github.com/lucidrains/vit-pytorch
  - https://github.com/kentaroy47/vision-transformers-cifar10
- https://github.com/Astle-sudo/Deep-Learning-C
