# 04_ai

## Founding papers

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)

## Courses

- [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html)
- [CS25: Transformers United V4](https://web.stanford.edu/class/cs25/)

## Interesting

- [What’s Really Going On in Machine Learning? Some Minimal Models](https://writings.stephenwolfram.com/2024/08/whats-really-going-on-in-machine-learning-some-minimal-models/)
- [Generative AI Space and the Mental Imagery of Alien Minds](https://writings.stephenwolfram.com/2023/07/generative-ai-space-and-the-mental-imagery-of-alien-minds/)
- [What Is ChatGPT Doing … and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)

- [Dive into Deep Learning](https://d2l.ai/index.html)
- [UvA Deep Learning Tutorials](https://uvadlc-notebooks.readthedocs.io/en/latest/)
- [Modern Approaches in Natural Language Processing](https://slds-lmu.github.io/seminar_nlp_ss20/)
- [Alternatives to a Degree to Prove Yourself in Deep Learning](https://www.fast.ai/posts/2017-04-06-alternatives.html)

- [Retrieval augmented generation: Keeping LLMs relevant and current](https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/)
- [Introduction to Weight Quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c)
  - FP32: 1 sign bit, 8 exponent bits, 23 significand bits
  - FP16: 1 sign bit, 5 exponent bits, 10 significand bits
  - BF16: 1 sign bit, 8 exponent bits, 7 significand bits -> prevent underflow,
    overflow of FP16
- [4-bit Quantization with GPTQ](https://towardsdatascience.com/4-bit-quantization-with-gptq-36b0f4f02c34)
- [Quantize Llama models with GGUF and llama.cpp](https://towardsdatascience.com/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172)
- [Quantization of LLMs with llama.cpp](https://medium.com/@ingridwickstevens/quantization-of-llms-with-llama-cpp-9bbf59deda35)
  - Q4_K_M: 4-bit quantization with K-means clustering with M aka medium size
- [How to Prune and Distill Llama-3.1 8B to an NVIDIA Llama-3.1-Minitron 4B Model](https://developer.nvidia.com/blog/how-to-prune-and-distill-llama-3-1-8b-to-an-nvidia-llama-3-1-minitron-4b-model/)

- [How I Use "AI"](https://nicholas.carlini.com/writing/2024/how-i-use-ai.html)
- https://www.reddit.com/r/MacOSBeta/comments/1ehivcp/macos_151_beta_1_apple_intelligence_backend/
- [I accidentally built a meme search engine](https://harper.blog/2024/04/12/i-accidentally-built-a-meme-search-engine/)

## Code pool

- [Tensors from Scratch #1: Tensors and their Shapes](https://maharshi.bearblog.dev/tensors-from-scratch-part-1/)
- [Tensors from Scratch #2: Elementwise operations and Broadcasting](https://maharshi.bearblog.dev/tensors-from-scratch-part-2/)
- [Informal approach to attention mechanisms](https://maharshi.bearblog.dev/informal-approach-to-attention-in-transformers/)
- [Generative transformer from first principles in Julia](https://liorsinai.github.io/machine-learning/2024/03/23/transformers-gpt.html)
- [The Path to StyleGan2 - Implementing the Progressive Growing GAN](https://ym2132.github.io/Progressive_GAN)

- https://github.com/lucidrains/vit-pytorch
  - https://github.com/kentaroy47/vision-transformers-cifar10
- https://github.com/Astle-sudo/Deep-Learning-C
